{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = sage.datasets.bank()\n",
    "\n",
    "# Feature names and categorical columns (for CatBoost model)\n",
    "feature_names = df.columns.tolist()[:-1]\n",
    "categorical_cols = ['Job', 'Marital', 'Education', 'Default', 'Housing',\n",
    "                    'Loan', 'Contact', 'Month', 'Prev Outcome']\n",
    "categorical_inds = [feature_names.index(col) for col in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train, test = train_test_split(\n",
    "    df.values, test_size=int(0.1 * len(df.values)), random_state=123)\n",
    "train, val = train_test_split(\n",
    "    train, test_size=int(0.1 * len(df.values)), random_state=123)\n",
    "Y_train = train[:, -1].copy().astype(int)\n",
    "Y_val = val[:, -1].copy().astype(int)\n",
    "Y_test = test[:, -1].copy().astype(int)\n",
    "train = train[:, :-1].copy()\n",
    "val = val[:, :-1].copy()\n",
    "test = test[:, :-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.mean(Y_train)\n",
    "pred = p.repeat(len(Y_test))\n",
    "pred = np.vstack((1 - pred, pred)).T\n",
    "base_loss = log_loss(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_list = []\n",
    "loss_list = []\n",
    "num_features = len(feature_names)\n",
    "num_subsets = 5000\n",
    "\n",
    "for i in range(num_subsets):\n",
    "    # Generate subset\n",
    "    num_included = np.random.choice(num_features + 1)\n",
    "    subset = np.zeros(num_features, dtype=bool)\n",
    "    subset[:num_included] = 1\n",
    "    np.random.shuffle(subset)\n",
    "    \n",
    "    # Subsample data\n",
    "    if num_included == 0:\n",
    "        loss = base_loss\n",
    "    else:\n",
    "        train_small = train[:, subset]\n",
    "        val_small = val[:, subset]\n",
    "        test_small = test[:, subset]\n",
    "        features = np.array(feature_names)[subset]\n",
    "        categorical_inds = [j for j in range(len(features)) if features[j] in categorical_cols]\n",
    "        \n",
    "        # Train model\n",
    "        model = CatBoostClassifier(iterations=100,\n",
    "                                   learning_rate=0.3,\n",
    "                                   depth=10)\n",
    "        model = model.fit(train_small, Y_train,\n",
    "                          categorical_inds,\n",
    "                          eval_set=(val_small, Y_val),\n",
    "                          verbose=False)\n",
    "        loss = log_loss(Y_test, model.predict_proba(test_small))\n",
    "\n",
    "    # Save result\n",
    "    loss_list.append(loss)\n",
    "    subset_list.append(subset)\n",
    "    \n",
    "    results_dict = {\n",
    "        'subsets': np.array(subset_list),\n",
    "        'loss': loss_list\n",
    "    }\n",
    "    with open('results/bank cumulative_correlation.pkl', 'wb') as f:\n",
    "        pickle.dump(results_dict, f)\n",
    "        \n",
    "    print('Done with {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage\n",
    "import pickle\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = sage.datasets.bike()\n",
    "feature_names = df.columns.tolist()[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data, with total count serving as regression target\n",
    "train, test = train_test_split(\n",
    "    df.values, test_size=int(0.1 * len(df.values)), random_state=123)\n",
    "train, val = train_test_split(\n",
    "    train, test_size=int(0.1 * len(df.values)), random_state=123)\n",
    "Y_train = train[:, -1].copy()\n",
    "Y_val = val[:, -1].copy()\n",
    "Y_test = test[:, -1].copy()\n",
    "train = train[:, :-3].copy()\n",
    "val = val[:, :-3].copy()\n",
    "test = test[:, :-3].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.mean(Y_train)\n",
    "pred = p.repeat(len(Y_test))\n",
    "base_loss = np.mean((Y_test - pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset_list = []\n",
    "loss_list = []\n",
    "num_features = len(feature_names)\n",
    "num_subsets = 5000\n",
    "\n",
    "for i in range(num_subsets):\n",
    "    # Generate subset\n",
    "    num_included = np.random.choice(num_features + 1)\n",
    "    subset = np.zeros(num_features, dtype=bool)\n",
    "    subset[:num_included] = 1\n",
    "    np.random.shuffle(subset)\n",
    "    \n",
    "    # Subsample data\n",
    "    if num_included == 0:\n",
    "        loss = base_loss\n",
    "    else:\n",
    "        train_small = train[:, subset]\n",
    "        val_small = val[:, subset]\n",
    "        test_small = test[:, subset]\n",
    "        dtrain = xgb.DMatrix(train_small, label=Y_train)\n",
    "        dval = xgb.DMatrix(val_small, label=Y_val)\n",
    "        dtest = xgb.DMatrix(test_small)\n",
    "        \n",
    "        # Train model\n",
    "        param = {\n",
    "            'max_depth' : 10,\n",
    "            'objective': 'reg:squarederror',\n",
    "            'nthread': 4\n",
    "        }\n",
    "        evallist = [(dtrain, 'train'), (dval, 'val')]\n",
    "        num_round = 50\n",
    "        model = xgb.train(param, dtrain, num_round, evallist, verbose_eval=False)\n",
    "        loss = np.mean((Y_test - model.predict(dtest)) ** 2)\n",
    "\n",
    "    # Save result\n",
    "    loss_list.append(loss)\n",
    "    subset_list.append(subset)\n",
    "    \n",
    "    results_dict = {\n",
    "        'subsets': np.array(subset_list),\n",
    "        'loss': loss_list\n",
    "    }\n",
    "    with open('results/bike cumulative_correlation.pkl', 'wb') as f:\n",
    "        pickle.dump(results_dict, f)\n",
    "        \n",
    "    print('Done with {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = sage.datasets.credit()\n",
    "\n",
    "# Feature names and categorical columns (for CatBoost model)\n",
    "feature_names = df.columns.tolist()[:-1]\n",
    "categorical_columns = [\n",
    "    'Checking Status', 'Credit History', 'Purpose', 'Credit Amount',\n",
    "    'Savings Account/Bonds', 'Employment Since', 'Personal Status',\n",
    "    'Debtors/Guarantors', 'Property Type', 'Other Installment Plans',\n",
    "    'Housing Ownership', 'Job', 'Telephone', 'Foreign Worker'\n",
    "]\n",
    "categorical_inds = [feature_names.index(col) for col in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train, test = train_test_split(\n",
    "    df.values, test_size=int(0.1 * len(df.values)), random_state=0)\n",
    "train, val = train_test_split(\n",
    "    train, test_size=int(0.1 * len(df.values)), random_state=0)\n",
    "Y_train = train[:, -1].copy().astype(int)\n",
    "Y_val = val[:, -1].copy().astype(int)\n",
    "Y_test = test[:, -1].copy().astype(int)\n",
    "train = train[:, :-1].copy()\n",
    "val = val[:, :-1].copy()\n",
    "test = test[:, :-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.mean(Y_train)\n",
    "pred = p.repeat(len(Y_test))\n",
    "pred = np.vstack((1 - pred, pred)).T\n",
    "base_loss = log_loss(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset_list = []\n",
    "loss_list = []\n",
    "num_features = len(feature_names)\n",
    "num_subsets = 5000\n",
    "\n",
    "for i in range(num_subsets):\n",
    "    # Generate subset\n",
    "    num_included = np.random.choice(num_features + 1)\n",
    "    subset = np.zeros(num_features, dtype=bool)\n",
    "    subset[:num_included] = 1\n",
    "    np.random.shuffle(subset)\n",
    "    \n",
    "    # Subsample data\n",
    "    if num_included == 0:\n",
    "        loss = base_loss\n",
    "    else:\n",
    "        train_small = train[:, subset]\n",
    "        val_small = val[:, subset]\n",
    "        test_small = test[:, subset]\n",
    "        features = np.array(feature_names)[subset]\n",
    "        categorical_inds = [j for j in range(len(features)) if features[j] in categorical_columns]\n",
    "        \n",
    "        # Train model\n",
    "        model = CatBoostClassifier(iterations=50,\n",
    "                                   learning_rate=0.3,\n",
    "                                   depth=3)\n",
    "        model = model.fit(train_small, Y_train,\n",
    "                          categorical_inds,\n",
    "                          eval_set=(val_small, Y_val),\n",
    "                          verbose=False)\n",
    "        loss = log_loss(Y_test, model.predict_proba(test_small))\n",
    "\n",
    "    # Save result\n",
    "    loss_list.append(loss)\n",
    "    subset_list.append(subset)\n",
    "    \n",
    "    results_dict = {\n",
    "        'subsets': np.array(subset_list),\n",
    "        'loss': loss_list\n",
    "    }\n",
    "    with open('results/credit cumulative_correlation.pkl', 'wb') as f:\n",
    "        pickle.dump(results_dict, f)\n",
    "        \n",
    "    print('Done with {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_names = [\n",
    "    'BCL11A', 'IGF1R', 'CCND1', 'CDK6', 'BRCA1', 'BRCA2', 'EZH2', 'SFTPD',\n",
    "    'CDC5L', 'ADMR', 'TSPAN2', 'EIF5B', 'ADRA2C', 'MRCL3', 'CCDC69', 'ADCY4',\n",
    "    'TEX14', 'RRM2B', 'SLC22A5', 'HRH1', 'SLC25A1', 'CEBPE', 'IWS1', 'FLJ10213',\n",
    "    'PSMD10', 'MARCH6', 'PDLIM4', 'SNTB1', 'CHCHD1', 'SCMH1', 'FLJ20489',\n",
    "    'MDP-1', 'FLJ30092', 'YTHDC2', 'LFNG', 'HOXD10', 'RPS6KA5', 'WDR40B',\n",
    "    'CST9L', 'ISLR', 'TMBIM1', 'TRABD', 'ARHGAP29', 'C15orf29', 'SCAMP4',\n",
    "    'TTC31', 'ZNF570', 'RAB42', 'SERPINI2', 'C9orf21'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "expression = pd.read_table('data/BRCA_TCGA_microarray.txt',\n",
    "                           sep='\\t', header=0,\n",
    "                           skiprows=lambda x: x == 1, index_col=0).T\n",
    "expression.index = pd.Index(\n",
    "    ['.'.join(sample.split('-')[:3]) for sample in expression.index])\n",
    "\n",
    "# Filter for reduced gene setif reduced:\n",
    "expression = expression[gene_names]\n",
    "\n",
    "# Impute missing values.\n",
    "expression = expression.fillna(expression.mean())\n",
    "\n",
    "# Load labels.\n",
    "labels = pd.read_table('data/TCGA_breast_type.tsv',\n",
    "                       sep='\\t', header=None,\n",
    "                       index_col=0, names=['Sample', 'Label'])\n",
    "\n",
    "# Filter for common samples.\n",
    "expression_index = expression.index.values\n",
    "labels_index = labels.index.values\n",
    "intersection = np.intersect1d(expression_index, labels_index)\n",
    "expression = expression.iloc[[i for i in range(len(expression))\n",
    "                              if expression_index[i] in intersection]]\n",
    "labels = labels.iloc[[i for i in range(len(labels))\n",
    "                      if labels_index[i] in intersection]]\n",
    "\n",
    "# Join expression data with labels.\n",
    "label_data = labels['Label'].values\n",
    "label_index = list(labels.index)\n",
    "expression['Label'] = np.array(\n",
    "    [label_data[label_index.index(sample)] for sample in expression.index])\n",
    "expression['Label'] = pd.Categorical(expression['Label']).codes\n",
    "data = expression.values\n",
    "\n",
    "# Split data\n",
    "train, test = train_test_split(\n",
    "    data, test_size=int(0.2 * len(data)), random_state=0)\n",
    "train, val = train_test_split(\n",
    "    train, test_size=int(0.2 * len(data)), random_state=0)\n",
    "Y_train = train[:, -1].copy().astype(int)\n",
    "Y_val = val[:, -1].copy().astype(int)\n",
    "Y_test = test[:, -1].copy().astype(int)\n",
    "train = train[:, :-1].copy()\n",
    "val = val[:, :-1].copy()\n",
    "test = test[:, :-1].copy()\n",
    "\n",
    "# Preprocess\n",
    "mean = train.mean(axis=0)\n",
    "std = train.std(axis=0)\n",
    "train = (train - mean) / std\n",
    "val = (val - mean) / std\n",
    "test = (test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression(train, Y_train, val, Y_val):\n",
    "    # Tune logistic regression model\n",
    "    C_list = np.arange(0.1, 1.0, 0.1)\n",
    "    best_loss = np.inf\n",
    "    best_C = None\n",
    "\n",
    "    for C in C_list:\n",
    "        # Fit model\n",
    "        model = LogisticRegression(C=C, penalty='l1', multi_class='multinomial',\n",
    "                                   solver='saga', max_iter=10000)\n",
    "        model.fit(train, Y_train)\n",
    "\n",
    "        # Calculate loss\n",
    "        train_loss = log_loss(Y_train, model.predict_proba(train))\n",
    "        val_loss = log_loss(Y_val, model.predict_proba(val))\n",
    "        # print('Train loss = {:.4f}, Val loss = {:.4f}'.format(train_loss, val_loss))\n",
    "\n",
    "        # See if best\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_C = C\n",
    "            \n",
    "    # Fit model on combined data\n",
    "    model = LogisticRegression(C=best_C, penalty='l1', multi_class='multinomial',\n",
    "                               solver='saga', max_iter=10000)\n",
    "    model.fit(np.concatenate((train, val), axis=0),\n",
    "              np.concatenate((Y_train, Y_val), axis=0))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [np.mean(Y_train == i) for i in range(4)]\n",
    "pred = np.array(p)[np.newaxis].repeat(len(Y_test), 0)\n",
    "base_loss = log_loss(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset_list = []\n",
    "loss_list = []\n",
    "num_features = len(gene_names)\n",
    "num_subsets = 5000\n",
    "\n",
    "for i in range(num_subsets):\n",
    "    # Generate subset\n",
    "    num_included = np.random.choice(num_features + 1)\n",
    "    subset = np.zeros(num_features, dtype=bool)\n",
    "    subset[:num_included] = 1\n",
    "    np.random.shuffle(subset)\n",
    "    \n",
    "    # Subsample data\n",
    "    if num_included == 0:\n",
    "        loss = base_loss\n",
    "    else:\n",
    "        train_small = train[:, subset]\n",
    "        val_small = val[:, subset]\n",
    "        test_small = test[:, subset]\n",
    "        \n",
    "        # Train model\n",
    "        model = fit_logistic_regression(train_small, Y_train, val_small, Y_val)\n",
    "        loss = log_loss(Y_test, model.predict_proba(test_small))\n",
    "\n",
    "    # Save result\n",
    "    loss_list.append(loss)\n",
    "    subset_list.append(subset)\n",
    "    \n",
    "    results_dict = {\n",
    "        'subsets': np.array(subset_list),\n",
    "        'loss': loss_list\n",
    "    }\n",
    "    with open('results/brca cumulative_correlation.pkl', 'wb') as f:\n",
    "        pickle.dump(results_dict, f)\n",
    "        \n",
    "    print('Done with {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.datasets as dsets\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train set\n",
    "train = dsets.MNIST('../data', train=True, download=True)\n",
    "imgs = train.data.reshape(-1, 784) / 255.0\n",
    "labels = train.targets\n",
    "\n",
    "# Shuffle and split into train and val\n",
    "inds = torch.randperm(len(train))\n",
    "imgs = imgs[inds]\n",
    "labels = labels[inds]\n",
    "val, Y_val = imgs[:6000], labels[:6000]\n",
    "train, Y_train = imgs[6000:], labels[6000:]\n",
    "\n",
    "# Load test set\n",
    "test = dsets.MNIST('../data', train=False, download=True)\n",
    "test, Y_test = test.data.reshape(-1, 784) / 255.0, test.targets\n",
    "\n",
    "# Move test data to numpy\n",
    "test_np = test.cpu().data.numpy()\n",
    "Y_test_np = Y_test.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train, Y_train, val, Y_val):\n",
    "    # Create model\n",
    "    device = torch.device('cuda', 2)\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(train.shape[1], 256),\n",
    "        nn.ELU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ELU(),\n",
    "        nn.Linear(256, 10)).to(device)\n",
    "\n",
    "    # Training parameters\n",
    "    lr = 1e-3\n",
    "    mbsize = 64\n",
    "    max_nepochs = 250\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    lookback = 5\n",
    "    verbose = False\n",
    "\n",
    "    # Move to GPU\n",
    "    train = train.to(device)\n",
    "    val = val.to(device)\n",
    "    # test = test.to(device)\n",
    "    Y_train = Y_train.to(device)\n",
    "    Y_val = Y_val.to(device)\n",
    "    # Y_test = Y_test.to(device)\n",
    "\n",
    "    # Data loader\n",
    "    train_set = TensorDataset(train, Y_train)\n",
    "    train_loader = DataLoader(train_set, batch_size=mbsize, shuffle=True)\n",
    "\n",
    "    # Setup\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    min_criterion = np.inf\n",
    "    min_epoch = 0\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(max_nepochs):\n",
    "        for x, y in train_loader:\n",
    "            # Move to device.\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            # Take gradient step.\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        # Check progress.\n",
    "        with torch.no_grad():\n",
    "            # Calculate validation loss.\n",
    "            val_loss = loss_fn(model(val), Y_val).item()\n",
    "            if verbose:\n",
    "                print('{}Epoch = {}{}'.format('-' * 10, epoch + 1, '-' * 10))\n",
    "                print('Val loss = {:.4f}'.format(val_loss))\n",
    "\n",
    "            # Check convergence criterion.\n",
    "            if val_loss < min_criterion:\n",
    "                min_criterion = val_loss\n",
    "                min_epoch = epoch\n",
    "                best_model = deepcopy(model)\n",
    "            elif (epoch - min_epoch) == lookback:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Keep best model\n",
    "    model = best_model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [np.mean(Y_train.data.numpy() == i) for i in range(10)]\n",
    "pred = np.array(p)[np.newaxis].repeat(len(Y_test), 0)\n",
    "base_loss = log_loss(Y_test_np, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda', 2)\n",
    "\n",
    "subset_list = []\n",
    "loss_list = []\n",
    "num_features = 784\n",
    "num_subsets = 5000\n",
    "\n",
    "for i in range(num_subsets):\n",
    "    # Generate subset\n",
    "    num_included = np.random.choice(num_features + 1)\n",
    "    subset = np.zeros(num_features, dtype=bool)\n",
    "    subset[:num_included] = 1\n",
    "    np.random.shuffle(subset)\n",
    "    \n",
    "    # Subsample data\n",
    "    if num_included == 0:\n",
    "        loss = base_loss\n",
    "    else:\n",
    "        train_small = train[:, subset]\n",
    "        val_small = val[:, subset]\n",
    "        test_small = test[:, subset]\n",
    "        \n",
    "        # Train model\n",
    "        model = train_model(train_small, Y_train, val_small, Y_val)\n",
    "        loss = log_loss(\n",
    "            Y_test,\n",
    "            model(test_small.to(device)).softmax(dim=1).cpu().data.numpy())\n",
    "\n",
    "    # Save result\n",
    "    loss_list.append(loss)\n",
    "    subset_list.append(subset)\n",
    "    \n",
    "    results_dict = {\n",
    "        'subsets': np.array(subset_list),\n",
    "        'loss': loss_list\n",
    "    }\n",
    "    with open('results/mnist cumulative_correlation.pkl', 'wb') as f:\n",
    "        pickle.dump(results_dict, f)\n",
    "        \n",
    "    print('Done with {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
