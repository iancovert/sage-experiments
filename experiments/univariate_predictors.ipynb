{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = sage.datasets.bank()\n",
    "\n",
    "# Feature names and categorical columns (for CatBoost model)\n",
    "feature_names = df.columns.tolist()[:-1]\n",
    "categorical_cols = ['Job', 'Marital', 'Education', 'Default', 'Housing',\n",
    "                    'Loan', 'Contact', 'Month', 'Prev Outcome']\n",
    "categorical_inds = [feature_names.index(col) for col in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train, test = train_test_split(\n",
    "    df.values, test_size=int(0.1 * len(df.values)), random_state=123)\n",
    "train, val = train_test_split(\n",
    "    train, test_size=int(0.1 * len(df.values)), random_state=123)\n",
    "Y_train = train[:, -1].copy().astype(int)\n",
    "Y_val = val[:, -1].copy().astype(int)\n",
    "Y_test = test[:, -1].copy().astype(int)\n",
    "train = train[:, :-1].copy()\n",
    "val = val[:, :-1].copy()\n",
    "test = test[:, :-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.mean(Y_train)\n",
    "pred = p.repeat(len(Y_test))\n",
    "pred = np.vstack((1 - pred, pred)).T\n",
    "base_loss = log_loss(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(len(feature_names))\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    # Subsample data\n",
    "    train_small = train[:, i:i+1]\n",
    "    val_small = val[:, i:i+1]\n",
    "    test_small = test[:, i:i+1]\n",
    "    feature_names_small = feature_names[i:i+1]\n",
    "    categorical_inds_small = [i for i in range(len(feature_names_small))\n",
    "                              if feature_names_small[i] in categorical_cols]\n",
    "    \n",
    "    # Train model\n",
    "    model = CatBoostClassifier(iterations=100,\n",
    "                               learning_rate=0.3,\n",
    "                               depth=10)\n",
    "    model = model.fit(train_small, Y_train,\n",
    "                      categorical_inds_small,\n",
    "                      eval_set=(val_small, Y_val),\n",
    "                      verbose=False)\n",
    "    \n",
    "    # Loss\n",
    "    loss = log_loss(Y_test, model.predict_proba(test_small))\n",
    "    scores[i] = base_loss - loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/bank univariate.pkl', 'wb') as f:\n",
    "    pickle.dump(scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = sage.datasets.bike()\n",
    "feature_names = df.columns.tolist()[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data, with total count serving as regression target\n",
    "train, test = train_test_split(\n",
    "    df.values, test_size=int(0.1 * len(df.values)), random_state=123)\n",
    "train, val = train_test_split(\n",
    "    train, test_size=int(0.1 * len(df.values)), random_state=123)\n",
    "Y_train = train[:, -1].copy()\n",
    "Y_val = val[:, -1].copy()\n",
    "Y_test = test[:, -1].copy()\n",
    "train = train[:, :-3].copy()\n",
    "val = val[:, :-3].copy()\n",
    "test = test[:, :-3].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.mean(Y_train)\n",
    "pred = p.repeat(len(Y_test))\n",
    "base_loss = np.mean((Y_test - pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(len(feature_names))\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    # Subsample data\n",
    "    inds = np.ones(len(feature_names), dtype=bool)\n",
    "    inds[i] = False\n",
    "    train_small = train[:, i:i+1]\n",
    "    val_small = val[:, i:i+1]\n",
    "    test_small = test[:, i:i+1]\n",
    "    dtrain = xgb.DMatrix(train_small, label=Y_train)\n",
    "    dval = xgb.DMatrix(val_small, label=Y_val)\n",
    "    dtest = xgb.DMatrix(test_small)\n",
    "    \n",
    "    # Train model\n",
    "    param = {\n",
    "        'max_depth' : 2,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'nthread': 4\n",
    "    }\n",
    "    evallist = [(dtrain, 'train'), (dval, 'val')]\n",
    "    num_round = 50\n",
    "    model = xgb.train(param, dtrain, num_round, evallist, verbose_eval=False)\n",
    "    \n",
    "    # Loss\n",
    "    loss = np.mean((model.predict(dtest) - Y_test) ** 2)\n",
    "    scores[i] = base_loss - loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/bike univariate.pkl', 'wb') as f:\n",
    "    pickle.dump(scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = sage.datasets.credit()\n",
    "\n",
    "# Feature names and categorical columns (for CatBoost model)\n",
    "feature_names = df.columns.tolist()[:-1]\n",
    "categorical_columns = [\n",
    "    'Checking Status', 'Credit History', 'Purpose', 'Credit Amount',\n",
    "    'Savings Account/Bonds', 'Employment Since', 'Personal Status',\n",
    "    'Debtors/Guarantors', 'Property Type', 'Other Installment Plans',\n",
    "    'Housing Ownership', 'Job', 'Telephone', 'Foreign Worker'\n",
    "]\n",
    "categorical_inds = [feature_names.index(col) for col in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train, test = train_test_split(\n",
    "    df.values, test_size=int(0.1 * len(df.values)), random_state=0)\n",
    "train, val = train_test_split(\n",
    "    train, test_size=int(0.1 * len(df.values)), random_state=0)\n",
    "Y_train = train[:, -1].copy().astype(int)\n",
    "Y_val = val[:, -1].copy().astype(int)\n",
    "Y_test = test[:, -1].copy().astype(int)\n",
    "train = train[:, :-1].copy()\n",
    "val = val[:, :-1].copy()\n",
    "test = test[:, :-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.mean(Y_train)\n",
    "pred = p.repeat(len(Y_test))\n",
    "pred = np.vstack((1 - pred, pred)).T\n",
    "base_loss = log_loss(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(len(feature_names))\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    # Subsample data\n",
    "    train_small = train[:, i:i+1]\n",
    "    val_small = val[:, i:i+1]\n",
    "    test_small = test[:, i:i+1]\n",
    "    feature_names_small = feature_names[i:i+1]\n",
    "    categorical_inds_small = [i for i in range(len(feature_names_small))\n",
    "                              if feature_names_small[i] in categorical_columns]\n",
    "    \n",
    "    # Train model\n",
    "    model = CatBoostClassifier(iterations=100,\n",
    "                               learning_rate=0.3,\n",
    "                               depth=10)\n",
    "    model = model.fit(train_small, Y_train,\n",
    "                      categorical_inds_small,\n",
    "                      eval_set=(val_small, Y_val),\n",
    "                      verbose=False)\n",
    "    \n",
    "    # Loss\n",
    "    loss = log_loss(Y_test, model.predict_proba(test_small))\n",
    "    scores[i] = base_loss - loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/credit univariate.pkl', 'wb') as f:\n",
    "    pickle.dump(scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_names = [\n",
    "    'BCL11A', 'IGF1R', 'CCND1', 'CDK6', 'BRCA1', 'BRCA2', 'EZH2', 'SFTPD',\n",
    "    'CDC5L', 'ADMR', 'TSPAN2', 'EIF5B', 'ADRA2C', 'MRCL3', 'CCDC69', 'ADCY4',\n",
    "    'TEX14', 'RRM2B', 'SLC22A5', 'HRH1', 'SLC25A1', 'CEBPE', 'IWS1', 'FLJ10213',\n",
    "    'PSMD10', 'MARCH6', 'PDLIM4', 'SNTB1', 'CHCHD1', 'SCMH1', 'FLJ20489',\n",
    "    'MDP-1', 'FLJ30092', 'YTHDC2', 'LFNG', 'HOXD10', 'RPS6KA5', 'WDR40B',\n",
    "    'CST9L', 'ISLR', 'TMBIM1', 'TRABD', 'ARHGAP29', 'C15orf29', 'SCAMP4',\n",
    "    'TTC31', 'ZNF570', 'RAB42', 'SERPINI2', 'C9orf21'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "expression = pd.read_table('data/BRCA_TCGA_microarray.txt',\n",
    "                           sep='\\t', header=0,\n",
    "                           skiprows=lambda x: x == 1, index_col=0).T\n",
    "expression.index = pd.Index(\n",
    "    ['.'.join(sample.split('-')[:3]) for sample in expression.index])\n",
    "\n",
    "# Filter for reduced gene setif reduced:\n",
    "expression = expression[gene_names]\n",
    "\n",
    "# Impute missing values.\n",
    "expression = expression.fillna(expression.mean())\n",
    "\n",
    "# Load labels.\n",
    "labels = pd.read_table('data/TCGA_breast_type.tsv',\n",
    "                       sep='\\t', header=None,\n",
    "                       index_col=0, names=['Sample', 'Label'])\n",
    "\n",
    "# Filter for common samples.\n",
    "expression_index = expression.index.values\n",
    "labels_index = labels.index.values\n",
    "intersection = np.intersect1d(expression_index, labels_index)\n",
    "expression = expression.iloc[[i for i in range(len(expression))\n",
    "                              if expression_index[i] in intersection]]\n",
    "labels = labels.iloc[[i for i in range(len(labels))\n",
    "                      if labels_index[i] in intersection]]\n",
    "\n",
    "# Join expression data with labels.\n",
    "label_data = labels['Label'].values\n",
    "label_index = list(labels.index)\n",
    "expression['Label'] = np.array(\n",
    "    [label_data[label_index.index(sample)] for sample in expression.index])\n",
    "expression['Label'] = pd.Categorical(expression['Label']).codes\n",
    "data = expression.values\n",
    "\n",
    "# Split data\n",
    "train, test = train_test_split(\n",
    "    data, test_size=int(0.2 * len(data)), random_state=0)\n",
    "train, val = train_test_split(\n",
    "    train, test_size=int(0.2 * len(data)), random_state=0)\n",
    "Y_train = train[:, -1].copy().astype(int)\n",
    "Y_val = val[:, -1].copy().astype(int)\n",
    "Y_test = test[:, -1].copy().astype(int)\n",
    "train = train[:, :-1].copy()\n",
    "val = val[:, :-1].copy()\n",
    "test = test[:, :-1].copy()\n",
    "\n",
    "# Preprocess\n",
    "mean = train.mean(axis=0)\n",
    "std = train.std(axis=0)\n",
    "train = (train - mean) / std\n",
    "val = (val - mean) / std\n",
    "test = (test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression(train, Y_train, val, Y_val):\n",
    "    # Tune logistic regression model\n",
    "    C_list = np.arange(0.1, 10.0, 0.1)\n",
    "    best_loss = np.inf\n",
    "    best_C = None\n",
    "\n",
    "    for C in C_list:\n",
    "        # Fit model\n",
    "        model = LogisticRegression(C=C, penalty='l1', multi_class='multinomial',\n",
    "                                   solver='saga', max_iter=10000)\n",
    "        model.fit(train, Y_train)\n",
    "\n",
    "        # Calculate loss\n",
    "        train_loss = log_loss(Y_train, model.predict_proba(train))\n",
    "        val_loss = log_loss(Y_val, model.predict_proba(val))\n",
    "        # print('Train loss = {:.4f}, Val loss = {:.4f}'.format(train_loss, val_loss))\n",
    "\n",
    "        # See if best\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_C = C\n",
    "            \n",
    "    # Fit model on combined data\n",
    "    model = LogisticRegression(C=best_C, penalty='l1', multi_class='multinomial',\n",
    "                               solver='saga', max_iter=10000)\n",
    "    model.fit(np.concatenate((train, val), axis=0),\n",
    "              np.concatenate((Y_train, Y_val), axis=0))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [np.mean(Y_train == i) for i in range(4)]\n",
    "pred = np.array(p)[np.newaxis].repeat(len(Y_test), 0)\n",
    "base_loss = log_loss(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(len(gene_names))\n",
    "\n",
    "for i in range(len(gene_names)):\n",
    "    # Subsample data\n",
    "    train_small = train[:, i:i+1]\n",
    "    val_small = val[:, i:i+1]\n",
    "    test_small = test[:, i:i+1]\n",
    "    \n",
    "    # Train model\n",
    "    model = fit_logistic_regression(train_small, Y_train, val_small, Y_val)\n",
    "    \n",
    "    # Loss\n",
    "    loss = log_loss(Y_test, model.predict_proba(test_small))\n",
    "    scores[i] = base_loss - loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/brca univariate.pkl', 'wb') as f:\n",
    "    pickle.dump(scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train set\n",
    "train = dsets.MNIST('../data', train=True, download=True)\n",
    "imgs = train.data.reshape(-1, 784) / 255.0\n",
    "labels = train.targets\n",
    "\n",
    "# Shuffle and split into train and val\n",
    "inds = torch.randperm(len(train))\n",
    "imgs = imgs[inds]\n",
    "labels = labels[inds]\n",
    "val, Y_val = imgs[:6000], labels[:6000]\n",
    "train, Y_train = imgs[6000:], labels[6000:]\n",
    "\n",
    "# Load test set\n",
    "test = dsets.MNIST('../data', train=False, download=True)\n",
    "test, Y_test = test.data.reshape(-1, 784) / 255.0, test.targets\n",
    "\n",
    "# Move test data to numpy\n",
    "test_np = test.cpu().data.numpy()\n",
    "Y_test_np = Y_test.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train, Y_train, val, Y_val):\n",
    "    # Create model\n",
    "    device = torch.device('cuda', 1)\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(train.shape[1], 32),\n",
    "        nn.ELU(),\n",
    "        nn.Linear(32, 10)).to(device)\n",
    "\n",
    "    # Training parameters\n",
    "    lr = 1e-3\n",
    "    mbsize = 64\n",
    "    max_nepochs = 250\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    lookback = 5\n",
    "    verbose = False\n",
    "\n",
    "    # Move to GPU\n",
    "    train = train.to(device)\n",
    "    val = val.to(device)\n",
    "    # test = test.to(device)\n",
    "    Y_train = Y_train.to(device)\n",
    "    Y_val = Y_val.to(device)\n",
    "    # Y_test = Y_test.to(device)\n",
    "\n",
    "    # Data loader\n",
    "    train_set = TensorDataset(train, Y_train)\n",
    "    train_loader = DataLoader(train_set, batch_size=mbsize, shuffle=True)\n",
    "\n",
    "    # Setup\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    min_criterion = np.inf\n",
    "    min_epoch = 0\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(max_nepochs):\n",
    "        for x, y in train_loader:\n",
    "            # Move to device.\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            # Take gradient step.\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        # Check progress.\n",
    "        with torch.no_grad():\n",
    "            # Calculate validation loss.\n",
    "            val_loss = loss_fn(model(val), Y_val).item()\n",
    "            if verbose:\n",
    "                print('{}Epoch = {}{}'.format('-' * 10, epoch + 1, '-' * 10))\n",
    "                print('Val loss = {:.4f}'.format(val_loss))\n",
    "\n",
    "            # Check convergence criterion.\n",
    "            if val_loss < min_criterion:\n",
    "                min_criterion = val_loss\n",
    "                min_epoch = epoch\n",
    "                best_model = deepcopy(model)\n",
    "            elif (epoch - min_epoch) == lookback:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Keep best model\n",
    "    model = best_model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [np.mean(Y_train.data.numpy() == i) for i in range(10)]\n",
    "pred = np.array(p)[np.newaxis].repeat(len(Y_test), 0)\n",
    "base_loss = log_loss(Y_test_np, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda', 1)\n",
    "scores = np.zeros(train.shape[1])\n",
    "\n",
    "for i in range(train.shape[1]):\n",
    "    # Subsample data\n",
    "    train_small = train[:, i:i+1]\n",
    "    val_small = val[:, i:i+1]\n",
    "    test_small = test[:, i:i+1]\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(train_small, Y_train, val_small, Y_val)\n",
    "    \n",
    "    # Loss\n",
    "    loss = log_loss(\n",
    "        Y_test_np,\n",
    "        model(test_small.to(device)).softmax(dim=1).cpu().data.numpy())\n",
    "    scores[i] = base_loss - loss\n",
    "    print('Done with {} (score = {:.4f})'.format(i, scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/mnist univariate.pkl', 'wb') as f:\n",
    "    pickle.dump(scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
