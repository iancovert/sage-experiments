{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage\n",
    "import pickle\n",
    "import baselines\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = sage.datasets.bike()\n",
    "feature_names = df.columns.tolist()[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data, with total count serving as regression target\n",
    "train, test = train_test_split(\n",
    "    df.values, test_size=int(0.1 * len(df.values)), random_state=123)\n",
    "train, val = train_test_split(\n",
    "    train, test_size=int(0.1 * len(df.values)), random_state=123)\n",
    "Y_train = train[:, -1].copy()\n",
    "Y_val = val[:, -1].copy()\n",
    "Y_test = test[:, -1].copy()\n",
    "train = train[:, :-3].copy()\n",
    "val = val[:, :-3].copy()\n",
    "test = test[:, :-3].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_models/bike model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(512):\n",
    "    bike_scores = baselines.permutation_test(\n",
    "        model, test, Y_test, 'mse', n_permutations=1)\n",
    "    bike_dict = {\n",
    "        'scores': bike_scores,\n",
    "        'evals': len(test)\n",
    "    }\n",
    "    with open('results/bike permutation_test {}.pkl'.format(i), 'wb') as f:\n",
    "        pickle.dump(bike_dict, f)\n",
    "    print('Done with {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage\n",
    "import pickle\n",
    "import baselines\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = sage.datasets.bank()\n",
    "\n",
    "# Feature names and categorical columns (for CatBoost model)\n",
    "feature_names = df.columns.tolist()[:-1]\n",
    "categorical_cols = ['Job', 'Marital', 'Education', 'Default', 'Housing',\n",
    "                    'Loan', 'Contact', 'Month', 'Prev Outcome']\n",
    "categorical_inds = [feature_names.index(col) for col in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train, test = train_test_split(\n",
    "    df.values, test_size=int(0.1 * len(df.values)), random_state=123)\n",
    "train, val = train_test_split(\n",
    "    train, test_size=int(0.1 * len(df.values)), random_state=123)\n",
    "Y_train = train[:, -1].copy().astype(int)\n",
    "Y_val = val[:, -1].copy().astype(int)\n",
    "Y_test = test[:, -1].copy().astype(int)\n",
    "train = train[:, :-1].copy()\n",
    "val = val[:, :-1].copy()\n",
    "test = test[:, :-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_models/bank model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(512):\n",
    "    bank_scores = baselines.permutation_test(\n",
    "        model, test, Y_test, 'cross entropy', n_permutations=1)\n",
    "    bank_dict = {\n",
    "        'scores': bank_scores,\n",
    "        'evals': len(test)\n",
    "    }\n",
    "    with open('results/bank permutation_test {}.pkl'.format(i), 'wb') as f:\n",
    "        pickle.dump(bank_dict, f)\n",
    "    print('Done with {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage\n",
    "import pickle\n",
    "import baselines\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = sage.datasets.credit()\n",
    "\n",
    "# Feature names and categorical columns (for CatBoost model)\n",
    "feature_names = df.columns.tolist()[:-1]\n",
    "categorical_columns = [\n",
    "    'Checking Status', 'Credit History', 'Purpose', 'Credit Amount',\n",
    "    'Savings Account/Bonds', 'Employment Since', 'Personal Status',\n",
    "    'Debtors/Guarantors', 'Property Type', 'Other Installment Plans',\n",
    "    'Housing Ownership', 'Job', 'Telephone', 'Foreign Worker'\n",
    "]\n",
    "categorical_inds = [feature_names.index(col) for col in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train, test = train_test_split(\n",
    "    df.values, test_size=int(0.1 * len(df.values)), random_state=0)\n",
    "train, val = train_test_split(\n",
    "    train, test_size=int(0.1 * len(df.values)), random_state=0)\n",
    "Y_train = train[:, -1].copy().astype(int)\n",
    "Y_val = val[:, -1].copy().astype(int)\n",
    "Y_test = test[:, -1].copy().astype(int)\n",
    "train = train[:, :-1].copy()\n",
    "val = val[:, :-1].copy()\n",
    "test = test[:, :-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_models/credit model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(512):\n",
    "    credit_scores = baselines.permutation_test(\n",
    "        model, test, Y_test, 'cross entropy', n_permutations=1)\n",
    "    credit_dict = {\n",
    "        'scores': credit_scores,\n",
    "        'evals': len(test)\n",
    "    }\n",
    "    with open('results/credit permutation_test {}.pkl'.format(i), 'wb') as f:\n",
    "        pickle.dump(credit_dict, f)\n",
    "    print('Done with {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage\n",
    "import pickle\n",
    "import baselines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_names = [\n",
    "    'BCL11A', 'IGF1R', 'CCND1', 'CDK6', 'BRCA1', 'BRCA2', 'EZH2', 'SFTPD',\n",
    "    'CDC5L', 'ADMR', 'TSPAN2', 'EIF5B', 'ADRA2C', 'MRCL3', 'CCDC69', 'ADCY4',\n",
    "    'TEX14', 'RRM2B', 'SLC22A5', 'HRH1', 'SLC25A1', 'CEBPE', 'IWS1', 'FLJ10213',\n",
    "    'PSMD10', 'MARCH6', 'PDLIM4', 'SNTB1', 'CHCHD1', 'SCMH1', 'FLJ20489',\n",
    "    'MDP-1', 'FLJ30092', 'YTHDC2', 'LFNG', 'HOXD10', 'RPS6KA5', 'WDR40B',\n",
    "    'CST9L', 'ISLR', 'TMBIM1', 'TRABD', 'ARHGAP29', 'C15orf29', 'SCAMP4',\n",
    "    'TTC31', 'ZNF570', 'RAB42', 'SERPINI2', 'C9orf21'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "expression = pd.read_table('data/BRCA_TCGA_microarray.txt',\n",
    "                           sep='\\t', header=0,\n",
    "                           skiprows=lambda x: x == 1, index_col=0).T\n",
    "expression.index = pd.Index(\n",
    "    ['.'.join(sample.split('-')[:3]) for sample in expression.index])\n",
    "\n",
    "# Filter for reduced gene setif reduced:\n",
    "expression = expression[gene_names]\n",
    "\n",
    "# Impute missing values.\n",
    "expression = expression.fillna(expression.mean())\n",
    "\n",
    "# Load labels.\n",
    "labels = pd.read_table('data/TCGA_breast_type.tsv',\n",
    "                       sep='\\t', header=None,\n",
    "                       index_col=0, names=['Sample', 'Label'])\n",
    "\n",
    "# Filter for common samples.\n",
    "expression_index = expression.index.values\n",
    "labels_index = labels.index.values\n",
    "intersection = np.intersect1d(expression_index, labels_index)\n",
    "expression = expression.iloc[[i for i in range(len(expression))\n",
    "                              if expression_index[i] in intersection]]\n",
    "labels = labels.iloc[[i for i in range(len(labels))\n",
    "                      if labels_index[i] in intersection]]\n",
    "\n",
    "# Join expression data with labels.\n",
    "label_data = labels['Label'].values\n",
    "label_index = list(labels.index)\n",
    "expression['Label'] = np.array(\n",
    "    [label_data[label_index.index(sample)] for sample in expression.index])\n",
    "expression['Label'] = pd.Categorical(expression['Label']).codes\n",
    "data = expression.values\n",
    "\n",
    "# Split data\n",
    "train, test = train_test_split(\n",
    "    data, test_size=int(0.2 * len(data)), random_state=0)\n",
    "train, val = train_test_split(\n",
    "    train, test_size=int(0.2 * len(data)), random_state=0)\n",
    "Y_train = train[:, -1].copy().astype(int)\n",
    "Y_val = val[:, -1].copy().astype(int)\n",
    "Y_test = test[:, -1].copy().astype(int)\n",
    "train = train[:, :-1].copy()\n",
    "val = val[:, :-1].copy()\n",
    "test = test[:, :-1].copy()\n",
    "\n",
    "# Preprocess\n",
    "mean = train.mean(axis=0)\n",
    "std = train.std(axis=0)\n",
    "train = (train - mean) / std\n",
    "val = (val - mean) / std\n",
    "test = (test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_models/brca model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(512):\n",
    "    brca_scores = baselines.permutation_test(\n",
    "        model, train, Y_train, 'cross entropy', n_permutations=1)\n",
    "    brca_dict = {\n",
    "        'scores': brca_scores,\n",
    "        'evals': len(train)\n",
    "    }\n",
    "    with open('results/brca permutation_test {}.pkl'.format(i), 'wb') as f:\n",
    "        pickle.dump(brca_dict, f)\n",
    "    print('Done with {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train set\n",
    "train = dsets.MNIST('../data', train=True, download=True)\n",
    "imgs = train.data.reshape(-1, 784) / 255.0\n",
    "labels = train.targets\n",
    "\n",
    "# Shuffle and split into train and val\n",
    "inds = torch.randperm(len(train))\n",
    "imgs = imgs[inds]\n",
    "labels = labels[inds]\n",
    "val, Y_val = imgs[:6000], labels[:6000]\n",
    "train, Y_train = imgs[6000:], labels[6000:]\n",
    "\n",
    "# Load test set\n",
    "test = dsets.MNIST('../data', train=False, download=True)\n",
    "test, Y_test = test.data.reshape(-1, 784) / 255.0, test.targets\n",
    "\n",
    "# Move test data to numpy\n",
    "test_np = test.cpu().data.numpy()\n",
    "Y_test_np = Y_test.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda', 2)\n",
    "model = torch.load('trained_models/mnist mlp.pt')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1024):\n",
    "    mnist_scores = baselines.permutation_test(\n",
    "        model, test_np, Y_test_np, 'cross entropy', n_permutations=1)\n",
    "    mnist_dict = {\n",
    "        'scores': mnist_scores,\n",
    "        'evals': len(test_np)\n",
    "    }\n",
    "    with open('results/mnist permutation_test {}.pkl'.format(i), 'wb') as f:\n",
    "        pickle.dump(mnist_dict, f)\n",
    "    print('Done with {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
